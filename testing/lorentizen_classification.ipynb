{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np  # noqa\n",
    "import pandas as pd  # noqa\n",
    "from pandas import DataFrame\n",
    "from freqtrade.persistence import Trade\n",
    "from typing import Optional\n",
    "from freqtrade.strategy import (IStrategy)\n",
    "\n",
    "# --------------------------------\n",
    "# Add your lib to import here\n",
    "import talib.abstract as ta\n",
    "import freqtrade.vendor.qtpylib.indicators as qtpylib\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from freqtrade.exchange import timeframe_to_prev_date\n",
    "from technical.util import resample_to_interval,resampled_merge\n",
    "import ccxt\n",
    "import math\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = ccxt.binance()\n",
    "timeframe = \"4h\"\n",
    "limit = 1000\n",
    "\n",
    "def ohlcv_info(ex:ccxt.Exchange,timeframe,limit):\n",
    "    x = ex.fetch_ohlcv('1000BONK/USDT:USDT', timeframe, limit=limit)\n",
    "    df = pd.DataFrame(x, columns=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df['date'] = pd.to_datetime(df['date'], unit='ms')\n",
    "    # print(f'*** {datetime.now()}  ***\\n {df.tail(10)}\\n')\n",
    "    return df\n",
    "dataframe = ohlcv_info(ex,timeframe,limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_CCI(dataframe,index,diff,ma,p):\n",
    "    close_array = dataframe['close'].to_numpy()\n",
    "    close_array = close_array[:index.name]\n",
    "    diff = diff.to_numpy()\n",
    "    diff = diff[index.name]\n",
    "    ma = ma.to_numpy()\n",
    "    ma = ma[index.name]\n",
    "\n",
    "    if(len(close_array) < p-1):\n",
    "        return np.nan\n",
    "\n",
    "    s = 0\n",
    "\n",
    "    for i in range(len(close_array),len(close_array)-p,-1):\n",
    "        s = s + abs(dataframe['close'][i] - ma)\n",
    "    mad = s / p\n",
    "\n",
    "    mcci = diff/mad/0.015\n",
    "    \n",
    "    return mcci\n",
    "\n",
    "def rescale(src, old_min, old_max, new_min, new_max):\n",
    "    return new_min + (new_max - new_min) * (src - old_min) / max((old_max - old_min), 10e-10)\n",
    "\n",
    "def normalize(value, min_val, max_val,df):\n",
    "    index = value.name\n",
    "    src = pd.Series(df[:index+1])\n",
    "\n",
    "    historic_min = 10e10\n",
    "    historic_max = -10e10\n",
    "\n",
    "    src_filled_min = src.fillna(historic_min)\n",
    "    src_filled_max = src.fillna(historic_max)\n",
    "    historic_min = min(src_filled_min.min(), historic_min) if not pd.isna(src_filled_min.min()) else historic_min\n",
    "    historic_max = max(src_filled_max.max(), historic_max) if not pd.isna(src_filled_max.max()) else historic_max\n",
    "\n",
    "    normalized_src = (min_val + (max_val - min_val) * (src[index] - historic_min)) / max((historic_max - historic_min), 10e-10)\n",
    "    return normalized_src\n",
    "\n",
    "def n_rsi(src, n1, n2):\n",
    "    rsi = ta.RSI(src,n1)\n",
    "    ema_rsi = ta.EMA(rsi,n2)\n",
    "    return rescale(ema_rsi, 0, 100, 0, 1)\n",
    "\n",
    "def n_cci(dataframe, n1 , n2):\n",
    "    df = dataframe.copy()\n",
    "    source = df['close']\n",
    "\n",
    "    df['mas'] = ta.SMA(source,n1)\n",
    "    df['diffs'] = source - df['mas']\n",
    "    df['cci'] = df.apply((lambda index: calculate_CCI(df,index,df['diffs'],df['mas'],n1)),axis=1)\n",
    "\n",
    "    df['ema_cci'] = ta.EMA(df['cci'],n2)\n",
    "\n",
    "    normalized_wt_diff = df.apply((lambda x : normalize(x,0,1,df['ema_cci'])),axis=1)\n",
    "    return normalized_wt_diff\n",
    "\n",
    "def n_wt(src, n1, n2):\n",
    "    df = pd.DataFrame({\"src\":src})\n",
    "    ema1 = ta.EMA(src, n1)\n",
    "    ema2 = ta.EMA(np.abs(src - ema1), n1)\n",
    "    ci = (src - ema1) / (0.015 * ema2)\n",
    "    wt1 = ta.EMA(ci, n2)\n",
    "    wt2 = ta.SMA(wt1, 4)\n",
    "    diff = wt1 - wt2\n",
    "    normalized_wt_diff = df.apply((lambda x : normalize(x,0,1,diff)),axis=1)\n",
    "    return normalized_wt_diff\n",
    "\n",
    "def calculate_tr(index,high,low,close):\n",
    "    high = pd.Series(high[:index+1])\n",
    "    low = pd.Series(low[:index+1])\n",
    "    close = pd.Series(close[:index+1])\n",
    "    prev_close = close.shift(1).fillna(0)\n",
    "\n",
    "    diff_h_n_l = high[index] - low[index]\n",
    "    abs_value_h_n_c = abs(high[index] - prev_close[index])\n",
    "    abs_value_l_n_c = abs(low[index] - prev_close[index])\n",
    "\n",
    "    tr = max(max(diff_h_n_l,abs_value_h_n_c),abs_value_l_n_c)\n",
    "    return tr\n",
    "\n",
    "def calculate_directionalMovementPlus(index,high,low):\n",
    "    high = pd.Series(high[:index+1])\n",
    "    low = pd.Series(low[:index+1])\n",
    "    prev_high = high.shift(1).fillna(0)\n",
    "    prev_low = low.shift(1).fillna(0)\n",
    "\n",
    "    diff_h_n_ph = high[index] - prev_high[index]\n",
    "    diff_pl_n_l = prev_low[index] - low[index]\n",
    "    dmp_value = max(diff_h_n_ph,0) if (diff_h_n_ph > diff_pl_n_l) else 0\n",
    "\n",
    "    return dmp_value\n",
    "\n",
    "def calculate_negMovement(index,high,low):\n",
    "    high = pd.Series(high[:index+1])\n",
    "    low = pd.Series(low[:index+1])\n",
    "    prev_high = high.shift(1).fillna(0)\n",
    "    prev_low = low.shift(1).fillna(0)\n",
    "\n",
    "    diff_h_n_ph = high[index] - prev_high[index]\n",
    "    diff_pl_n_l = prev_low[index] - low[index]\n",
    "    negMovement = max(diff_pl_n_l,0) if (diff_pl_n_l > diff_h_n_ph) else 0\n",
    "    return negMovement\n",
    "\n",
    "def n_adx(highSrc, lowSrc, closeSrc,dataframe, n1):\n",
    "    df = dataframe.copy()\n",
    "    length = n1\n",
    "    th = 20\n",
    "    tr = df.apply((lambda x : calculate_tr(x.name,highSrc,lowSrc,closeSrc)),axis=1)\n",
    "    directionalMovementPlus = df.apply((lambda x : calculate_directionalMovementPlus(x.name,highSrc,lowSrc)),axis=1)\n",
    "    negMovement = df.apply((lambda x : calculate_negMovement(x.name,highSrc,lowSrc)),axis=1)\n",
    "\n",
    "    trSmooth = np.zeros_like(closeSrc)\n",
    "    trSmooth[0] = np.nan\n",
    "    for i in range(0, len(tr)):\n",
    "        trSmooth[i] = trSmooth[i-1] - trSmooth[i-1] / length + tr[i]\n",
    "\n",
    "    smoothDirectionalMovementPlus = np.zeros_like(closeSrc)\n",
    "    smoothDirectionalMovementPlus[0] = np.nan\n",
    "    for i in range(0, len(directionalMovementPlus)):\n",
    "        smoothDirectionalMovementPlus[i] = smoothDirectionalMovementPlus[i-1] - smoothDirectionalMovementPlus[i-1] / length + directionalMovementPlus[i]\n",
    "\n",
    "    smoothnegMovement = np.zeros_like(closeSrc)\n",
    "    smoothnegMovement[0] = np.nan\n",
    "    for i in range(0, len(negMovement)):\n",
    "        smoothnegMovement[i] = smoothnegMovement[i-1] - smoothnegMovement[i-1] / length + negMovement[i]\n",
    "    \n",
    "    diPositive = smoothDirectionalMovementPlus / trSmooth * 100\n",
    "    diNegative = smoothnegMovement / trSmooth * 100\n",
    "    dx = np.abs(diPositive - diNegative) / (diPositive + diNegative) * 100\n",
    "    dx_series = pd.Series(dx)\n",
    "\n",
    "    adx = dx_series.copy()\n",
    "    adx.iloc[:length] = adx.rolling(length).mean().iloc[:length]\n",
    "    adx = adx.ewm(alpha=(1.0/length),adjust=False).mean()\n",
    "    return rescale(adx, 0, 100, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heikinashi(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_HA = df.copy()\n",
    "    df_HA['close'] = (df_HA['open'] + df_HA['high'] +\n",
    "                      df_HA['low'] + df_HA['close']) / 4\n",
    "\n",
    "    for i in range(0, len(df_HA)):\n",
    "        if i == 0:\n",
    "            df_HA.loc[i, 'open'] = (\n",
    "                (df_HA.loc[i, 'open'] + df_HA.loc[i, 'close']) / 2)\n",
    "        else:\n",
    "            df_HA.loc[i, 'open'] = (\n",
    "                (df_HA.loc[i-1, 'open'] + df_HA.loc[i-1, 'close']) / 2)\n",
    "\n",
    "    df_HA['high'] = df_HA[['open', 'close', 'high']].max(axis=1)\n",
    "    df_HA['low'] = df_HA[['open', 'close', 'low']].min(axis=1)\n",
    "\n",
    "    return df_HA\n",
    "\n",
    "\n",
    "class FeatureName(Enum):\n",
    "    rsi = \"RSI\"\n",
    "    wt = \"WT\"\n",
    "    cci = \"CCI\"\n",
    "    adx = \"ADX\"\n",
    "\n",
    "\n",
    "def chooseFeatureName(name: FeatureName, dataframe, paramsA, paramsB):\n",
    "    df = dataframe.copy()\n",
    "    source = df['close']\n",
    "    hlc3 = (df['high'] + df['low'] + df['close']) / 3\n",
    "\n",
    "    if (name.name == FeatureName.rsi.name):\n",
    "        return n_rsi(source, paramsA, paramsB)\n",
    "    if (name.name == FeatureName.wt.name):\n",
    "        return n_wt(hlc3, paramsA, paramsB)\n",
    "    if (name.name == FeatureName.cci.name):\n",
    "        return n_cci(df, paramsA, paramsB)\n",
    "    if (name.name == FeatureName.adx.name):\n",
    "        return n_adx(df['high'], df['low'], df['close'], df, paramsA)\n",
    "\n",
    "\n",
    "def extract_features(dataframe: pd.DataFrame, training_params):\n",
    "    df = dataframe.copy()\n",
    "    ha_dataframe = heikinashi(df)\n",
    "\n",
    "    f1_name = training_params['f1']['name']\n",
    "    f1_param_A = training_params['f1']['paramsA']\n",
    "    f1_param_B = training_params['f1']['paramsB']\n",
    "\n",
    "    f2_name = training_params['f2']['name']\n",
    "    f2_param_A = training_params['f2']['paramsA']\n",
    "    f2_param_B = training_params['f2']['paramsB']\n",
    "\n",
    "    f3_name = training_params['f3']['name']\n",
    "    f3_param_A = training_params['f3']['paramsA']\n",
    "    f3_param_B = training_params['f3']['paramsB']\n",
    "\n",
    "    f4_name = training_params['f4']['name']\n",
    "    f4_param_A = training_params['f4']['paramsA']\n",
    "    f4_param_B = training_params['f4']['paramsB']\n",
    "\n",
    "    f5_name = training_params['f5']['name']\n",
    "    f5_param_A = training_params['f5']['paramsA']\n",
    "    f5_param_B = training_params['f5']['paramsB']\n",
    "\n",
    "    df['f1'] = chooseFeatureName(f1_name, ha_dataframe, f1_param_A, f1_param_B)\n",
    "    df['f2'] = chooseFeatureName(f2_name, ha_dataframe, f2_param_A, f2_param_B)\n",
    "    df['f3'] = chooseFeatureName(f3_name, ha_dataframe, f3_param_A, f3_param_B)\n",
    "    df['f4'] = chooseFeatureName(f4_name, ha_dataframe, f4_param_A, f4_param_B)\n",
    "    df['f5'] = chooseFeatureName(f5_name, ha_dataframe, f5_param_A, f5_param_B)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_volatility(dataframe:pd.DataFrame,minLength:int,maxLength:int):\n",
    "    df = dataframe.copy()\n",
    "    recentAtr = ta.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=minLength)\n",
    "    historicalAtr = ta.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=maxLength)\n",
    "    return recentAtr > historicalAtr\n",
    "\n",
    "def regime_filter(dataframe, threshold):\n",
    "    df = dataframe.copy()\n",
    "    ohlc4 = (df['open'] + df['high'] + df['low'] + df['close']) / 4\n",
    "    src = ohlc4\n",
    "\n",
    "    value1 = pd.Series(0,index=df['close'].index, dtype=float)\n",
    "    value2 = pd.Series(0,index=df['close'].index, dtype=float)\n",
    "    klmf = pd.Series(0,index=df['close'].index, dtype=float)\n",
    "\n",
    "    for i in range(0, len(value1)):\n",
    "        if(i == 0):\n",
    "            value1[i] = 0\n",
    "        else:\n",
    "            value1[i] = 0.2 * (src[i] - src[i-1]) + 0.8 * value1[i-1]\n",
    "\n",
    "    for i in range(0, len(value1)):\n",
    "        if(i == 0):\n",
    "            value2[i] = 0.1 * (df['high'][i] - df['low'][i]) + 0.8 * 0\n",
    "        else:\n",
    "            value2[i] = 0.1 * (df['high'][i] - df['low'][i]) + 0.8 * value2[i-1]\n",
    "\n",
    "    omega = abs(value1 / value2)\n",
    "    alpha = (-omega**2 + np.sqrt(omega**4 + 16 * omega**2)) / 8 \n",
    "\n",
    "    for i in range(0, len(value1)):\n",
    "        if(i == 0):\n",
    "            klmf[i] = alpha[i] * src[i] + (1 - alpha[i]) * 0\n",
    "        else:\n",
    "            klmf[i] = alpha[i] * src[i] + (1 - alpha[i]) * klmf[i-1]\n",
    "\n",
    "    absCurveSlope = klmf.diff().abs()\n",
    "    exponentialAverageAbsCurveSlope = 1.0 * ta.EMA(absCurveSlope, 200)\n",
    "    normalized_slope_decline = (absCurveSlope - exponentialAverageAbsCurveSlope) / exponentialAverageAbsCurveSlope\n",
    "    return normalized_slope_decline >= threshold\n",
    "\n",
    "def ema_filter(dataframe,period):\n",
    "    df = dataframe.copy()\n",
    "    ema = ta.EMA(df['close'], period)\n",
    "    filter_value = (df['close'] > ema).astype(int) - (df['close'] < ema).astype(int)\n",
    "    return filter_value\n",
    "\n",
    "def sma_filter(dataframe,period):\n",
    "    df = dataframe.copy()\n",
    "    sma = ta.SMA(df['close'], period)\n",
    "    filter_value = (df['close'] > sma).astype(int) - (df['close'] < sma).astype(int)\n",
    "    return filter_value\n",
    "\n",
    "def kernel_filter(dataframe,loopback,relative_weight,start_at_bar):\n",
    "    df = dataframe.copy()\n",
    "    khat1 = pd.Series(rational_quadratic(df['close'], loopback, relative_weight, start_at_bar))\n",
    "    # wasBearishRate = khat1.shift(2) > khat1.shift(1)\n",
    "    # isBearishRate = khat1.shift(1) > khat1\n",
    "    # wasBullishRate = khat1.shift(2) < khat1.shift(1)\n",
    "    filter_rate = (khat1.shift(1) < khat1).astype(int) - (khat1.shift(1) > khat1).astype(int)\n",
    "    return filter_rate\n",
    "\n",
    "def rational_quadratic(\n",
    "    price_feed: np.ndarray,\n",
    "    lookback: int,\n",
    "    relative_weight: float,\n",
    "    start_at_bar: int,\n",
    ") -> np.ndarray:\n",
    "    length_of_prices = len(price_feed)\n",
    "    bars_calculated = start_at_bar + 1\n",
    "\n",
    "    result = np.zeros(length_of_prices, dtype=float)\n",
    "    lookback_squared = np.power(lookback, 2)\n",
    "    denominator = lookback_squared * 2 * relative_weight\n",
    "\n",
    "    for index in range(length_of_prices):\n",
    "        current_weight = 0.0\n",
    "        cumulative_weight = 0.0\n",
    "\n",
    "        for i in range(bars_calculated):\n",
    "            y = np.nan if (index - i) < 0 else price_feed[index - i]\n",
    "            w = np.power(\n",
    "                1 + (np.power(i, 2) / denominator),\n",
    "                -relative_weight,\n",
    "            )\n",
    "            current_weight += y * w\n",
    "            cumulative_weight += w\n",
    "\n",
    "        result[index] = current_weight / cumulative_weight\n",
    "\n",
    "    return result\n",
    "\n",
    "def gaussian(\n",
    "    price_feed: np.ndarray,\n",
    "    lookback: int,\n",
    "    start_at_bar: int,\n",
    ") -> np.ndarray:\n",
    "    length_of_prices = len(price_feed)\n",
    "    bars_calculated = start_at_bar + 1\n",
    "\n",
    "    result = np.zeros(length_of_prices, dtype=float)\n",
    "    lookback_squared = np.power(lookback, 2)\n",
    "    denominator = lookback_squared * 2\n",
    "\n",
    "    for index in range(length_of_prices):\n",
    "        current_weight = 0.0\n",
    "        cumulative_weight = 0.0\n",
    "\n",
    "        for i in range(bars_calculated):\n",
    "            y = np.nan if (index - i) < 0 else price_feed[index - i]\n",
    "            w = np.exp(-(np.power(i, 2) / denominator))\n",
    "            current_weight += y * w\n",
    "            cumulative_weight += w\n",
    "\n",
    "        result[index] = current_weight / cumulative_weight\n",
    "\n",
    "    return result\n",
    "\n",
    "class Filter(Enum):\n",
    "    volatility = \"filter_volatility\"\n",
    "    regime = \"regime_filter\"\n",
    "    ema = \"ema_filter\"\n",
    "    sma = \"sma_filter\"\n",
    "    kernel = \"kernel_filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLorentizanDistance(i,current_feature,feature_array):\n",
    "    feature_distance = math.log(1+ abs(current_feature - feature_array[i]))\n",
    "    return feature_distance\n",
    "\n",
    "def fractalFilters(predict_value:pd.Series):\n",
    "    isDifferentSignalType = predict_value.ne(predict_value.shift())\n",
    "    return isDifferentSignalType\n",
    "\n",
    "def compare_value(index,length,value):\n",
    "    df = value\n",
    "    prev_df = value.shift(length)\n",
    "    if(prev_df[index] < df[index]):\n",
    "        return -1\n",
    "    elif(prev_df[index] > df[index]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def setPredictionAsClearWay(index,dataframe:pd.DataFrame,filter_method):\n",
    "    df = dataframe.copy()\n",
    "    global signal_predictions\n",
    "    prediction_value = 0\n",
    "    predicted_value = df['predicted_value'].iloc[index]\n",
    "    filter_value = True\n",
    "\n",
    "    for filter in filter_method:\n",
    "        if((filter.name == \"ema\") or (filter.name == \"sma\") or (filter.name == \"kernel\")):\n",
    "            filter_value = True and filter_value\n",
    "        else:\n",
    "            filter_value = (df[filter.value].iloc[index]) and (filter_value)\n",
    "\n",
    "    if (predicted_value > 0) & filter_value:\n",
    "        prediction_value = 1\n",
    "    elif (predicted_value < 0) & filter_value:\n",
    "        prediction_value = -1\n",
    "    else:\n",
    "        if index == 0:\n",
    "            prediction_value = 0\n",
    "        else:\n",
    "            prediction_value = signal_predictions[index-1]\n",
    "    signal_predictions[index] = prediction_value\n",
    "    return prediction_value\n",
    "\n",
    "def train_model(index,df,training_params):\n",
    "    current_index = index\n",
    "    lastDistance = -1.0\n",
    "    neighbour_count = training_params['neighbor_count']\n",
    "    feature_count = training_params['feature_count']\n",
    "    # Variable Used for ML\n",
    "    global distances\n",
    "    global predictions\n",
    "\n",
    "    feature_array_1 = df['f1'].to_numpy()\n",
    "    feature_array_2 = df['f2'].to_numpy()\n",
    "    feature_array_3 = df['f3'].to_numpy()\n",
    "    feature_array_4 = df['f4'].to_numpy()\n",
    "    feature_array_5 = df['f5'].to_numpy()\n",
    "    y_train_array = df['y_train'].to_numpy()\n",
    "\n",
    "    current_feature_1 =  feature_array_1[current_index]\n",
    "    current_feature_2 =  feature_array_2[current_index]\n",
    "    current_feature_3 =  feature_array_3[current_index]\n",
    "    current_feature_4 =  feature_array_4[current_index]\n",
    "    current_feature_5 =  feature_array_5[current_index] \n",
    "\n",
    "    feature_array_1 = feature_array_1[:current_index+1]\n",
    "    feature_array_2 = feature_array_2[:current_index+1]\n",
    "    feature_array_3 = feature_array_3[:current_index+1]\n",
    "    feature_array_4 = feature_array_4[:current_index+1]\n",
    "    feature_array_5 = feature_array_5[:current_index+1]\n",
    "\n",
    "    y_train_array = y_train_array[:current_index+1]\n",
    "\n",
    "    for i in range(0,current_index+1,1):\n",
    "        d = 0\n",
    "        current_feature_names = [current_feature_1, current_feature_2,current_feature_3,current_feature_4,current_feature_5]\n",
    "        feature_array_names = [feature_array_1, feature_array_2,feature_array_3,feature_array_4,feature_array_5]\n",
    "        current_feature_names = current_feature_names[:feature_count]\n",
    "        feature_array_names = feature_array_names[:feature_count]\n",
    "\n",
    "        for var_index,_ in enumerate(current_feature_names):\n",
    "            current_feature_count = current_feature_names[var_index]\n",
    "            feature_array_count = feature_array_names[var_index]\n",
    "            d = getLorentizanDistance(i,current_feature_count,feature_array_count) + d\n",
    "        \n",
    "        if (d >= lastDistance) and (i%4):\n",
    "            lastDistance = d\n",
    "            distances.append(d)\n",
    "            predictions.append(round(y_train_array[i]))\n",
    "            if len(predictions) > neighbour_count:\n",
    "                lastDistance = distances[round(neighbour_count*3/4)]\n",
    "                distances.pop(0)\n",
    "                predictions.pop(0)\n",
    "    \n",
    "    prediction = sum(predictions)\n",
    "    return prediction\n",
    "\n",
    "def predict_future(dataframe:pd.DataFrame,training_params):\n",
    "    df = dataframe.copy()\n",
    "    global signal_predictions\n",
    "    signal_predictions = {}\n",
    "    filter_method = training_params['filter_method']\n",
    "    future_count = training_params['future_count']\n",
    "\n",
    "    df['y_train'] = df.apply(lambda x: compare_value(x.name,future_count,df['close']),axis=1)\n",
    "    df['predicted_value'] = df.apply((lambda x : train_model(x.name,df,training_params)),axis=1)\n",
    "    df['final_prediction'] = df.apply(lambda x : setPredictionAsClearWay(x.name,df,filter_method),axis=1)\n",
    "    df['isDifferentSignalType'] = fractalFilters(df['final_prediction'])\n",
    "\n",
    "    dataframe['buy_signal'] = (df['final_prediction'] > 0) & (df['isDifferentSignalType'])\n",
    "    dataframe['sell_signal'] = (df['final_prediction'] < 0) & (df['isDifferentSignalType'])\n",
    "\n",
    "    for filter in filter_method:\n",
    "        value = df[filter.value]\n",
    "        if((filter.name == \"ema\") or (filter.name == \"sma\") or (filter.name == \"kernel\")):\n",
    "            dataframe['buy_signal'] = dataframe['buy_signal'] & (value > 0)\n",
    "            dataframe['sell_signal'] = dataframe['sell_signal'] & (value < 0)\n",
    "\n",
    "    return dataframe\n",
    "# No Filter buy_27 sell_27\n",
    "# All buy_4 sell_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy_signal\n",
      "False    796\n",
      "True      25\n",
      "Name: count, dtype: int64\n",
      "sell_signal\n",
      "False    796\n",
      "True      25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def mlRunModel(dataframe,training_params):\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    kernel_loopback = training_params['filter_params']['kernel']['loopback']\n",
    "    kernel_relative_weight = training_params['filter_params']['kernel']['weighting']\n",
    "    kernel_start_at_bar = training_params['filter_params']['kernel']['regression_level']\n",
    "\n",
    "    df = extract_features(df,training_params)\n",
    "\n",
    "    df['filter_volatility'] = filter_volatility(df,1,10)\n",
    "    df['regime_filter'] = regime_filter(df,training_params['filter_params']['regime']['threshold'])\n",
    "    df['ema_filter'] = ema_filter(df,training_params['filter_params']['ema']['threshold'])\n",
    "    df['sma_filter'] = sma_filter(df,training_params['filter_params']['sma']['threshold'])\n",
    "    df['kernel_filter'] = kernel_filter(df,kernel_loopback,kernel_relative_weight,kernel_start_at_bar)\n",
    "\n",
    "    df = predict_future(df,training_params)\n",
    "    return df\n",
    "\n",
    "distances = []\n",
    "predictions = []\n",
    "signal_predictions = {}\n",
    "training_params = {\n",
    "    \"filter_method\" : [], #done\n",
    "    \"filter_params\" : {\n",
    "        \"kernel\" : {\n",
    "            \"loopback\" : 8,\n",
    "            \"weighting\" : 8,\n",
    "            \"regression_level\" : 25\n",
    "        },\n",
    "        \"regime\" : {\n",
    "            \"threshold\" : -0.1\n",
    "        },\n",
    "        \"ema\" : {\n",
    "            \"threshold\" : 200\n",
    "        },\n",
    "        \"sma\" : {\n",
    "            \"threshold\" : 200\n",
    "        }\n",
    "    },\n",
    "    \"neighbor_count\" : 8, #done\n",
    "    \"feature_count\" : 5, #done\n",
    "    \"future_count\" : 4, #done\n",
    "    \"f1\" : {\n",
    "        \"name\" : FeatureName.rsi,\n",
    "        \"paramsA\" : 14,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f2\" : {\n",
    "        \"name\" : FeatureName.wt,\n",
    "        \"paramsA\" : 10,\n",
    "        \"paramsB\" : 11\n",
    "    },\n",
    "    \"f3\" : {\n",
    "        \"name\" : FeatureName.cci,\n",
    "        \"paramsA\" : 20,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f4\" : {\n",
    "        \"name\" : FeatureName.adx,\n",
    "        \"paramsA\" : 20,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "    \"f5\" : {\n",
    "        \"name\" : FeatureName.rsi,\n",
    "        \"paramsA\" : 9,\n",
    "        \"paramsB\" : 2\n",
    "    },\n",
    "}\n",
    "\n",
    "dataframe = mlRunModel(dataframe,training_params)\n",
    "print(dataframe['buy_signal'].value_counts())\n",
    "print(dataframe['sell_signal'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
